/* ----------------------------------------------------------------------
   LAMMPS - Large-scale Atomic/Molecular Massively Parallel Simulator
   https://www.lammps.org/, Sandia National Laboratories
   LAMMPS development team: developers@lammps.org

   Copyright (2003) Sandia Corporation.  Under the terms of Contract
   DE-AC04-94AL85000 with Sandia Corporation, the U.S. Government retains
   certain rights in this software.  This software is distributed under
   the GNU General Public License.

   See the README file in the top-level LAMMPS directory.
------------------------------------------------------------------------- */

#include "lammps.h"

#include "exceptions.h"
#include "input.h"
#include "library.h"

#include "json.h"

#include <cstdlib>
#include <iostream>  // Added for std::cout
#include <mpi.h>
#include <new>

// import MolSSI Driver Interface library
#if defined(LMP_MDI)
#include <mdi.h>
#endif

using namespace LAMMPS_NS;

// for convenience
static void finalize()
{
  lammps_kokkos_finalize();
  lammps_python_finalize();
  lammps_plugin_finalize();
}

/* ----------------------------------------------------------------------
   main program to drive LAMMPS
------------------------------------------------------------------------- */

int main(int argc, char **argv)
{
  // âœ… CRITICAL: Set JAX ROCm environment BEFORE any LAMMPS/Python initialization
  std::cout << "ðŸ”§ LAMMPS main.cpp: Setting JAX ROCm environment..." << std::endl;
  
  // ROCm-specific environment variables (changed from CUDA)
  setenv("JAX_PLATFORMS", "rocm", 1);
  setenv("HIP_VISIBLE_DEVICES", "0", 1);                // AMD equivalent of CUDA_VISIBLE_DEVICES
  setenv("ROCR_VISIBLE_DEVICES", "0", 1);               // ROCm runtime device selection
  setenv("HSA_OVERRIDE_GFX_VERSION", "9.4.2", 1);      // MI300A architecture
  setenv("XLA_PYTHON_CLIENT_PREALLOCATE", "false", 1);
  setenv("XLA_FLAGS", "--xla_gpu_autotune_level=4 --xla_gpu_enable_async_collectives=true", 1);
  
  // Additional ROCm optimizations
  setenv("HIP_LAUNCH_BLOCKING", "0", 1);               // Allow async kernel launches
  setenv("ROCM_PATH", "/opt/rocm", 1);                 // ROCm installation path
  setenv("HCC_AMDGPU_TARGET", "gfx942", 1);           // MI300A architecture
  
  // Debug: Verify environment was set
  const char* jax_platforms = getenv("JAX_PLATFORMS");
  const char* hip_devices = getenv("HIP_VISIBLE_DEVICES");
  const char* rocr_devices = getenv("ROCR_VISIBLE_DEVICES");
  const char* gfx_version = getenv("HSA_OVERRIDE_GFX_VERSION");
  
  std::cout << "âœ… JAX_PLATFORMS: " << (jax_platforms ? jax_platforms : "NOT_SET") << std::endl;
  std::cout << "âœ… HIP_VISIBLE_DEVICES: " << (hip_devices ? hip_devices : "NOT_SET") << std::endl;
  std::cout << "âœ… ROCR_VISIBLE_DEVICES: " << (rocr_devices ? rocr_devices : "NOT_SET") << std::endl;
  std::cout << "âœ… HSA_OVERRIDE_GFX_VERSION: " << (gfx_version ? gfx_version : "NOT_SET") << std::endl;
  std::cout << "ðŸš€ Starting LAMMPS with ROCm environment pre-configured..." << std::endl;

  MPI_Init(&argc, &argv);
  MPI_Comm lammps_comm = MPI_COMM_WORLD;

#if defined(LMP_MDI)
  // initialize MDI interface, if compiled in

  int mdi_flag;
  if (MDI_Init(&argc, &argv)) MPI_Abort(MPI_COMM_WORLD, 1);
  if (MDI_Initialized(&mdi_flag)) MPI_Abort(MPI_COMM_WORLD, 1);

  // get the MPI communicator that spans all ranks running LAMMPS
  // when using MDI, this may be a subset of MPI_COMM_WORLD

  if (mdi_flag)
    if (MDI_MPI_get_world_comm(&lammps_comm)) MPI_Abort(MPI_COMM_WORLD, 1);
#endif

  try {
    auto *lammps = new LAMMPS(argc, argv, lammps_comm);
    lammps->input->file();
    delete lammps;
  } catch (LAMMPSAbortException &ae) {
    finalize();
    MPI_Abort(ae.get_universe(), 1);
  } catch (LAMMPSException &) {
    finalize();
    MPI_Barrier(lammps_comm);
    MPI_Finalize();
    exit(1);
  } catch (fmt::format_error &fe) {
    fprintf(stderr, "\nfmt::format_error: %s%s\n", fe.what(), utils::errorurl(12).c_str());
    finalize();
    MPI_Abort(MPI_COMM_WORLD, 1);
    exit(1);
  } catch (json::exception &je) {
    fprintf(stderr, "\nJSON library error %d: %s\n", je.id, je.what());
    finalize();
    MPI_Abort(MPI_COMM_WORLD, 1);
    exit(1);
  } catch (std::bad_alloc &ae) {
    fprintf(stderr, "C++ memory allocation failed: %s\n", ae.what());
    finalize();
    MPI_Abort(MPI_COMM_WORLD, 1);
    exit(1);
  } catch (std::exception &e) {
    fprintf(stderr, "Exception: %s\n", e.what());
    finalize();
    MPI_Abort(MPI_COMM_WORLD, 1);
    exit(1);
  }
  finalize();
  MPI_Barrier(lammps_comm);
  MPI_Finalize();
  
  std::cout << "âœ… LAMMPS completed successfully" << std::endl;
  return 0;
}
